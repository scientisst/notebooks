{"cells": [{"cell_type": "markdown", "metadata": {}, "source": [" <div  style=\"color:#303030;font-family:'arial blACK', sans-serif,monospace; text-align: center; padding: 50px 0; vertical-align:middle;\" > <img src=\"https://github.com/PIA-Group/ScientIST-notebooks/blob/master/_Resources/Images/Lightbulb.png?raw=true\" style=\" background:linear-gradient(to right,#FDC86E,#fbb144);border-radius:10px;width:150px;text-align:left; margin-left:10%\"  /> <span style=\"position:relative; bottom:70px; margin-left:5%;font-size:170%;\">    Classification of Human Activity Data </span> </div>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## <span style=\"color:#fbb144;\"> Keywords: </span>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["```Supervised Learning```,```Unsupervised Learning```, ```Accelerometry Data```"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# I. Introduction\n", "<br>\n", "<div style=\"width:100%; background:linear-gradient(to right,#FDC86E,#fbb144);font-family:'arial black',monospace; text-align: center; padding: 7px 0; border-radius: 5px 50px;margin-top:-15px\" >  </div>\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In this work, we propose a framework for the automatic classification of 4 different human activity movements (right arm elevation at 90\u00ba and 180\u00ba on the coronal and sagittal planes) using accelerometer data acquired from a smarthphone hold on the hand. We do so by applying supervised and unsupervised machine learning algorithms."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<img src=\"https://github.com/PIA-Group/ScientIST-notebooks/blob/master/_Resources/Images/E.Classification_IMG/e003/arm-raise-new.png?raw=true\" alt=\"arm-raise\" border=\"0\"/> "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## <div style=\"color:#fbb144;\"> 1. Objectives</div>\n", "* Learn to segment motion signals\n", "* Extract meaningful information from accelerometer\n", "* Perform activity recognition through simple clustering"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# II. Experimental\n", "<br>\n", "<div style=\"width:100%; background:linear-gradient(to right,#FDC86E,#fbb144);font-family:'arial black',monospace; text-align: center; padding: 7px 0; border-radius: 5px 50px;margin-top:-15px\" >  </div>\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## <div style=\"color:#fbb144;\">  1. Requirements</div>\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In this section, the libraries required should be installed, using the command:"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["%matplotlib notebook\n", "\n", "!pip install sklearn >/dev/null 2>&1\n", "!pip install scipy >/dev/null 2>&1\n", "!pip install biosppy >/dev/null 2>&1"]}, {"cell_type": "markdown", "metadata": {}, "source": ["and imported:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# NumPy is the fundamental package for scientific computing with Python. \n", "import numpy as np \n", "# Matplotlib is a Python 2D plotting library\n", "import matplotlib.pyplot as plt\n", "# to help dealing with directories\n", "import os \n", "# to manage tables\n", "import pandas as pd \n", "# to create dictionaries in alphanetical order\n", "from collections import OrderedDict \n", "# metrics to apply to time series\n", "import scipy \n", "# to load and save files keeping the original format\n", "import pickle "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## <div style=\"color:#fbb144;\">  2. Data</div>\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## <div style=\"color:#fbb144;\">  2.1. Load the data </div>\n", "The data for this experiment was previously aquired by 1 user performing 180\u00ba and 90\u00ba arm movements on the coronal and transversal planes. Each activity was saved in a different csv file, which can be read as a dictionary. In order to speed the next steps, all activities will be joined together in the same file. If you prefer, this new file can be saved in the format \"pickle\"."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### <div style=\"color:#fbb144;\">2.1.1. Load csv files and create dictionary </div>\n", "\n", "Csv files can be easily loaded with pandas function \"read_csv\", which allows to keep the original table structure since it is saved in a DataFrame. To join all DataFrames together, the dictionary type is also a simple way to keep the original structure intact. Each dataframe is saved in the dictionary using the file name as key. For the following steps to work, correct each file names to include one of the following \"C 180\", \"C 90\", \"S 180\" and \"S 90\"."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["## GET CSV FILES DIRECTORY\n", "\n", "#place your file directory here\n", "directory = '' \n", "files_dir = os.listdir(directory)\n", "\n", "# list only csv files\n", "csv_files = [file for file in files_dir if file.endswith('.csv')] \n", "\n", "\n", "## LOAD FILES AND SAVE IN A DICTIONARY\n", "data = "]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div style=\"background:#48ba57;font-family:'arial', monospace; text-align: center; padding: 10px 0; border-radius:10px; width:70%; margin:auto \" >\n", "  <span style=\"font-size:20px;position:relative;color:white; \">  Note </span> <br>\n", "  <div style=\"background:#9de3a6;font-size:12px\"> \n", "    List comprehensions are a compact way of writing loops. In the previous cell we list the file names and create the dictionary with all files packed together using list comprehensions. Find out more here: https://docs.python.org/3/tutorial/datastructures.html   \n", "</div>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### <div style=\"color:#fbb144;\">2.1.2. Save and load pickle files </div>\n", "The data for this experiment was previously aquired by 1 user performing 180\u00ba and 90\u00ba arm movements on the coronal and transversal planes. Each activity was saved in a different csv file, which can be read as a dictionary. In order to speed the next steps, all activities will be joined together in the same file. If you prefer, this new file can be saved in the format \"pickle\"."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\n", "new_directory = ''\n", "\n", "pickle.dump(data, open(new_directory, 'wb'))\n", "\n", "#pickle.load(open(new_directory, 'rb'))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data[csv_files[0]]['AccX'].values"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["activities_dir = list(data.keys()) #activities names (this should be the same as csv_files)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## <div style=\"color:#fbb144;\">  2.2. Data Reconstruction</div>\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div style=\"background:#fbb144;font-family:'arial', monospace; text-align: center; padding: 10px 0; border-radius:10px; width:70%; margin:auto \" >\n", "  <span style=\"font-size:20px;position:relative;color:white; \">  Warning! </span> <br>\n", "  <div style=\"background:#ffd08a;font-size:12px\"> \n", "    The data acquired through Google Science Journal was not perfectly acquired, thus it is necessary to compute missing values. For that, we first remove all 'NaN' and then the signal is reconstructed based on the length and the existing values. On the following plots we show the comparison between the original data and reconstructed data.   \n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["colors = ['#00bfc2','#5756d6','#fada5e', '#62d321', '#fe9b29']\n", "\n", "from scipy.signal import resample\n", "for act in activities_dir:\n", "    a = 0\n", "    \n", "    plt.figure(figsize=(15,5))\n", "    for axis in data[act].columns:\n", "        a+=1\n", "        plt.suptitle(act)\n", "        plt.subplot(2,5,a+5)\n", "        # copy signal\n", "        original_sig = data[act][axis] \n", "        \n", "        data[act][axis] = resample(data[act].dropna(subset=[axis])[axis].values, len(data[act][axis])) # resample \n", "        plt.plot(data[act][axis], color = colors[0], label='Reconstructed')\n", "        plt.legend()\n", "        plt.subplot(2,5,a)\n", "        plt.title(axis)\n", "        plt.plot(original_sig, color = colors[1], label='Original')\n", "        plt.legend()\n", "    plt.show()\n", "        "]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div style=\"background:#48ba57;font-family:'arial', monospace; text-align: center; padding: 10px 0; border-radius:10px; width:70%; margin:auto \" >\n", "  <span style=\"font-size:20px;position:relative;color:white; \">  Note </span> <br>\n", "  <div style=\"background:#9de3a6;font-size:12px\"> \n", "    Since we replace the original signal by the reconstructed, you need to play the previous cell again, to read the original signal and do its reconstruction. Otherwise, both plots will show the same signal.\n", "</div>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## <div style=\"color:#fbb144;\">  2.3. Data Segmentation </div>\n", "\n", "Only use axis 'AccX', 'AccY' and 'AccZ'. These signals will be segmented through 'AccY'. The activity starts when the axis Y reaches its minimum value and it stops when the maximum of each cycle is reached. In the plot below the activitied are segmented between the vertical purple lines.\n", "\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div style=\"background:#fe9b29;font-family:'arial', monospace; text-align: center; padding: 10px 0; border-radius:10px; width:70%; margin:auto \" >\n", "  <span style=\"font-size:20px;position:relative;color:white; \">  Caution! </span> <br>\n", "  <div style=\"background:#ffdab0;font-size:12px\"> \n", "    The following code might need some adjustments in the distance parameter, in order to find all peaks. If this value is too large, some peaks will not be found and to many peaks will be found if this parameter is to small. Check in the plot if all peaks are correctly found.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["segment_idx = OrderedDict()\n", "i = 1\n", "plt.figure(figsize=(20,10))\n", "\n", "for act in activities_dir:\n", "    x = data[act]['AccY']\n", "    max_points = scipy.signal.find_peaks(x, height=np.mean(x), threshold=None, distance=100, prominence=None, \n", "                                         width=None, wlen=None, rel_height=0.5, plateau_size=None)[0]\n", "    min_points = scipy.signal.find_peaks(x*(-1), height=np.mean(-x), threshold=None, distance=30, prominence=None, \n", "                                         width=None, wlen=None, rel_height=0.5, plateau_size=None)[0]\n", "    plt.subplot(2,2,i)\n", "    i += 1\n", "    plt.plot(data[act]['AccY'], color=colors[0])\n", "    plt.title(act)\n", "    mp_new = []\n", "    for mx in max_points:\n", "        min_point = min_points[np.argmin([abs(mp-mx) for mp in min_points[np.argwhere(min_points<mx)]])]\n", "        mp_new += [min_point]\n", "    print(len(max_points), len(mp_new))\n", "        \n", "    \n", "    segment_idx[act] = np.vstack([mp_new, max_points]).T\n", "    if act == activities_dir[1]:\n", "        segment_idx[act] = segment_idx[act][:-1]\n", "    plt.vlines(segment_idx[act].ravel(), np.min(x), np.max(x), color=colors[1])\n", "plt.show()\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["segment_idx[act]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["segmented_data = OrderedDict()\n", "for act in activities_dir:\n", "    segmented_data[act] = pd.DataFrame()\n", "    for axis in data[act].columns:\n", "        segments = []\n", "        for idx in segment_idx[act]:\n", "        # run segments idx \n", "            #save here the cropped data\n", "            segments  += [data[act][axis][idx[0]:idx[1]]] \n", "            \n", "            plt.plot(segments[-1], color=colors[0])\n", "            plt.xlabel('Time (s)', color=\"#00a0e4\")\n", "            plt.ylabel('Amplitude (m/s^2)', color=\"#00a0e4\")\n", "        plt.title(axis)\n", "        plt.show()\n", "        #save here the segments of each activity and axis\n", "        segmented_data[act][axis] = segments \n", "            "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## <div style=\"color:#fbb144\">   3.1. Feature Extraction </div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["segmented_data[act][axis][0]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["feats_data = OrderedDict()\n", "feats = ['mean', 'median', 'max', 'var', 'std_dev', 'abs_dev', 'kurtosis', 'skewness']\n", "\n", "import biosppy as bs\n", "for act in activities_dir:\n", "    for axis in ['AccX', 'AccY', 'AccZ']:\n", "        feats_val = []\n", "        feats_axis = [axis +'-'+ ft for ft in feats]\n", "        for seg in range(len(segmented_data[act][axis])):\n", "            # get statistical features using biosppy.signals.tools.signal_stats]\n", "            feats_val += [bs.signals.tools.signal_stats(segmented_data[act][axis][seg])[:]] \n", "        if axis == 'AccX':\n", "            # save as dataframe\n", "            new_row = pd.DataFrame(feats_val, columns=feats_axis) \n", "        else:\n", "            # add new features to dataframe\n", "            new_row = pd.concat([new_row, pd.DataFrame(feats_val, columns=feats_axis)], axis=1)\n", "        \n", "    feats_data[act] = new_row # save the dataframe here"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div style=\"background:#48ba57;font-family:'arial', monospace; text-align: center; padding: 10px 0; border-radius:10px; width:70%; margin:auto \" >\n", "  <span style=\"font-size:20px;position:relative;color:white; \">  Note </span> <br>\n", "  <div style=\"background:#9de3a6;font-size:12px\"> \n", "    Even though we have five columns, we chose to use only AccX, AccY and AccZ. While column relative_time is not informative, column DecibelSource has wrong information, since the activities performed are not sound dependent, sound was used as a marker for segmentation. Still it was more reliable to segment through the well defined AccY, than through sound.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["feats_data"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": ["print('Features for activity ', activities_dir[1])\n", "feats_data[activities_dir[1]]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## <div style=\"color:#fbb144;\">   3.2. Feature Selection </div>\n", "\n", "A set of 30 features might not be a lot for some classification task, however, since we have a very short amount of samples, we could run into overfitting issues. There are several ways to decide which features might be best for a particular problem. An empirical overview (by observing) the behaviour of each feature in the different activities is a good starting point."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for feat in feats_data[act].columns:\n", "    ci = 0\n", "    for activity in feats_data.keys():\n", "        plt.plot(feats_data[activity][feat], label=activity, color=colors[ci])\n", "        ci+=1\n", "    plt.title(feat)\n", "    plt.legend()\n", "    plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["best_features = []\n", "for act in feats_data.keys():\n", "    feats_data[act] = feats_data[act][best_features]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div style=\"background:#946db2;font-family:'arial', monospace; text-align: center; padding: 10px 0; border-radius:10px; width:70%; margin:auto \" >\n", "  <span style=\"font-size:20px;position:relative;color:white; \"> Explore </span> <br>\n", "  <div style=\"background:#d0b3e6;font-size:12px\"> \n", "    Explore diferent forms of feature selection. For more information go to: https://scikit-learn.org/stable/modules/feature_selection.html\n", "        </div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["feats_data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Create an X and Y joining all information from feats_data. To ease reading, we will replace the activities' names for a simpler expression 'C_180', 'C_90', 'S_180' and 'S_90'."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# stack all features together\n", "X = np.vstack([feats_data[act] for act in feats_data.keys()])\n", "#act_ = #simplify activities labels\n", "act_ = {activities_dir[0]: 'C_180', activities_dir[1]: 'C_90', activities_dir[2]: 'S_180',\n", "       activities_dir[3]: 'S_90'}\n", "Y = np.hstack([[act_[act]]*len(feats_data[act]) for act in feats_data.keys()])\n", "#Y = #stack all labels together using the new simplified names of each class\n", "Y #show Y"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div style=\"background:#48ba57;font-family:'arial', monospace; text-align: center; padding: 10px 0; border-radius:10px; width:70%; margin:auto \" >\n", "  <span style=\"font-size:20px;position:relative;color:white; \">  Note </span> <br>\n", "  <div style=\"background:#9de3a6;font-size:12px\"> \n", "    Alternatively, the dataset could be separated into a training (learn the model), validation (tunn hyperparemeters) and test set (evaluate the model).\n", "</div>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div style=\"background:#946db2;font-family:'arial', monospace; text-align: center; padding: 10px 0; border-radius:10px; width:70%; margin:auto \" >\n", "  <span style=\"font-size:20px;position:relative;color:white; \"> Explore </span> <br>\n", "  <div style=\"background:#d0b3e6;font-size:12px\"> \n", "    Explore diferent forms of train-test separation and cross-validation. For more information go to: https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation\n", "        </div>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div style=\"background:#946db2;font-family:'arial', monospace; text-align: center; padding: 10px 0; border-radius:10px; width:70%; margin:auto \" >\n", "  <span style=\"font-size:20px;position:relative;color:white; \"> Explore </span> <br>\n", "  <div style=\"background:#d0b3e6;font-size:12px\"> \n", "    Explore how different test sizes impact the classification results. \n", "</div>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div style=\"background:#fbb144;font-family:'arial', monospace; text-align: center; padding: 10px 0; border-radius:10px; width:70%; margin:auto \" >\n", "  <span style=\"font-size:20px;position:relative;color:white; \">  Warning! </span> <br>\n", "  <div style=\"background:#ffd08a;font-size:12px\"> \n", "    When the data is highly imbalanced, use the stratify hyperparemeter set to the ground-truth labels so the data is proportionally distributed per class in the training and test sets.\n", "</div>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "## <div style=\"color:#fbb144\">   4.1.   K-Means </div>\n", "\n", "The proximity between groups with the same degree compared to the distance between the two different degrees hampers the separation of the 4 clusters at the same time. Thus it is more reliable to separate firstly in two big clusters and then separate again inside each cluster individually.\n", "\n", "### <div style=\"color:#fbb144\">   4.1.1.   Clustering using K-means </div>\n", "The KMeans algorithm clusters data by trying to separate samples in n groups of equal variance, minimizing a criterion known as the inertia or within-cluster sum-of-squares (see below). This algorithm requires the number of clusters to be specified.\n", "\n", "Create a KMeans cluster algorithm by calling <b>KMeans()</b> and hosting it in the <b>kmeans</b> variable. Define the number of clusters.\n", "\n", "Required Hyperparameters:\n", "\n", "* <b>n_clusters</b> int, default=8\n", "The number of clusters to form as well as the number of centroids to generate.\n", "\n", "For more information regarding the method go to https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# import kmeans package\n", "from sklearn.cluster import KMeans \n", "# call KMeans cluster method, defining the number of clusters\n", "kmeans = "]}, {"cell_type": "markdown", "metadata": {}, "source": ["Compute cluster centers and predict cluster index for each sample by calling  <b>fit_predict(self, X[, y, sample_weight])</b>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false}, "outputs": [], "source": ["y_pred ="]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_pred"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# plot result\n", "for pred in range(len(X)):\n", "    plt.scatter(X[pred][1], X[pred][3], color=colors[y_pred[pred]], label=Y[pred])\n", "plt.legend(bbox_to_anchor=(1.1, 1), ncol=4)\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["i=1\n", "plt.figure(figsize=(15,15))\n", "\n", "plt.subplots_adjust(wspace=0.4, hspace=0.3)\n", "\n", "for bf in range(len(best_features)):\n", "    for bi in range(len(best_features)):\n", "        if bf != bi:\n", "            plt.subplot(5, 4, i)\n", "            for pred in range(len(X)):\n", "                plt.scatter(X[pred][bf], X[pred][bi], color=colors[y_pred[pred]], label=Y[pred])\n", "                plt.ylabel(best_features[bf])\n", "                plt.title(best_features[bi])\n", "\n", "            i+=1\n", "\n", "plt.legend(bbox_to_anchor=(1.1, 1), ncol=4)\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import v_measure_score \n", "\n", "print('Score: ', v_measure_score(Y, y_pred))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div style=\"background:#946db2;font-family:'arial', monospace; text-align: center; padding: 10px 0; border-radius:10px; width:70%; margin:auto \" >\n", "  <span style=\"font-size:20px;position:relative;color:white; \"> Explore </span> <br>\n", "  <div style=\"background:#d0b3e6;font-size:12px\"> \n", "    To know which features might be more appropriate, we can see which are linearly separable in the following plots. Change list best_features to those who seem to achieve the best separation.    \n", "</div>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div style=\"background:#946db2;font-family:'arial', monospace; text-align: center; padding: 10px 0; border-radius:10px; width:70%; margin:auto \" >\n", "  <span style=\"font-size:20px;position:relative;color:white; \"> Explore </span> <br>\n", "  <div style=\"background:#d0b3e6;font-size:12px\"> \n", "    Explore defining some of Kmeans hyperparameters and the inpact on the clustering results. For more information go to: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n", "</div>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div style=\"background:#946db2;font-family:'arial', monospace; text-align: center; padding: 10px 0; border-radius:10px; width:70%; margin:auto \" >\n", "  <span style=\"font-size:20px;position:relative;color:white; \"> Explore </span> <br>\n", "  <div style=\"background:#d0b3e6;font-size:12px\"> \n", "    Explore different clustering algorithms. For more information go to: https://scikit-learn.org/stable/modules/clustering.html\n", "</div>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## <div style=\"color:#fbb144;\">   5 Supervised Learning </div>\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## <div style=\"color:#fbb144;\"> Train-test split\n", "</div>\n", "\n", "Split arrays or matrices into random train and test subsets using the method <b>train_test_split</b> https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n", "\n", "Input Paremeters:\n", "* <b>arrays</b> sequence of indexables with same length / shape[0]\n", "Allowed inputs are lists, numpy arrays, scipy-sparse matrices or pandas dataframes.\n", "\n", "\n", "* <b>test_size</b> float or int, default=None\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# import package\n", "from sklearn.model_selection import train_test_split  \n", "\n", "# train_test_split of X and Y with test size 30%\n", "X_train, X_test, y_train, y_test = "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## <div style=\"color:#fbb144;\">   5.1.  Decision Tree </div>\n", "\n", "<b>DecisionTreeClassifier</b> is a class capable of performing multi-class classification on a dataset.\n", "\n", "As with other classifiers, DecisionTreeClassifier takes as input two arrays: an array X, sparse or dense, of size [n_samples, n_features] holding the training samples, and an array Y of integer values, size [n_samples], holding the class labels for the training samples:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### <div style=\"color:#fbb144;\">   5.1.1. Create Decision Tree </div>\n", "\n", "Create a Decision Tree classifier by calling <b>tree.DecisionTreeClassifier()</b> and hosting it in the <b>DT</b> variable.\n", "\n", "For more information regarding the classifier go to https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# import package\n", "from sklearn import tree \n", "\n", "# call decision tree classifier\n", "clf = "]}, {"cell_type": "markdown", "metadata": {}, "source": ["### <div style=\"color:#fbb144;\">   5.1.2. Train Decision Tree </div>\n", "\n", "Build a decision tree classifier from the training set (X_train, y_train) by calling the method <b>fit(self, X, y[, sample_weight, \u2026])</b> on the decision tree classifier."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# train the classifier using fit(X_train, y_train)\n", "clf = "]}, {"cell_type": "markdown", "metadata": {}, "source": ["After being fitted, the model can then be used to predict the class of samples.\n", "\n", "Use the method <b>predict(self, X[, check_input])</b> to do so."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# use the classifier to infer predictions using predict(X_test)  # Predict class or regression value for X_test.\n", "\n", "y_predicted = \n", "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_predicted).sum()))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_predicted"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Observe the Decision Tree classifier predictions agains the ground-truth labels.\n", "\n", "To do so, use the function <b>classification_report</b> to build a text report showing the main classification metrics.\n", "\n", "Input Paremeters:\n", "* <b>y_true</b> 1d array-like, or label indicator array / sparse matrix\n", "Ground truth (correct) target values.\n", "\n", "* <b>y_pred</b> 1d array-like, or label indicator array / sparse matrix\n", "Estimated targets as returned by a classifier."]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false}, "outputs": [], "source": ["from sklearn.metrics import classification_report\n", "\n", "# classification report\n", "#print(classification_report(y_test, y_predicted))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div style=\"background:#48ba57;font-family:'arial', monospace; text-align: center; padding: 10px 0; border-radius:10px; width:70%; margin:auto \" >\n", "  <span style=\"font-size:20px;position:relative;color:white; \">  Note </span> <br>\n", "  <div style=\"background:#9de3a6;font-size:12px\"> \n", "    In the case of highly highly imbalanced data, the F1-score, precision and recall per class are more informative than the accuracy score, which does not take into consideration the class imbalance.\n", "</div>\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import confusion_matrix\n", "import seaborn as sb\n", "\n", "def plot_confusion_matrix(y_true, y_pred, true_labels=None, normalize=True, title=''):\n", "    \"\"\"\n", "\n", "    :param y_true: type(array), contains true labels\n", "    :param y_pred: type(array), contains predicted labels\n", "    :param true_labels: list of unique labels\n", "    :param normalize: boolean\n", "    :return:\n", "    \"\"\"\n", "    if true_labels is None:\n", "        true_labels = np.unique(y_true)\n", "    cm = confusion_matrix(y_true, y_pred, labels=true_labels)  # TODO\n", "    if normalize:\n", "        cm = np.round(cm / np.sum(cm, axis=1), 2)\n", "    plt.figure(figsize=(10,5))\n", "    ax = plt.subplot(1,1,1)\n", "    ax.set_title(title)\n", "    # annot=True to annotate cells\n", "    sb.heatmap(cm, annot=True, ax=ax, fmt='g', cmap='Blues')  \n", "    # labels, title and ticks\n", "    ax.set_xlabel('Predicted', fontsize=20)\n", "    ax.xaxis.set_label_position('top')\n", "    ax.xaxis.set_ticklabels(true_labels, fontsize=10)\n", "    ax.xaxis.tick_top()\n", "    ax.set_ylabel('True', fontsize=20)\n", "    ax.yaxis.set_ticklabels(true_labels, fontsize=10)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Compute a confusion matrix to evaluate the accuracy of a classification in a table format. \n", "\n", "Do so by calling the <b>plot_confusion_matrix</b> function.\n", "\n", "Input parameters:\n", "\n", "* <b>y_true</b>: type(array), contains true labels\n", "\n", "* <b>y_pred</b>: type(array), contains predicted labels\n", "\n", "* <b>true_labels</b>: list of unique labels\n", "\n", "* <b>normalize</b>: boolean"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": ["# Confusion Matrix\n", "target_names = np.unique(y_test)\n", "plot_confusion_matrix(y_test, y_predicted, true_labels=np.unique(y_test), normalize=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div style=\"background:#48ba57;font-family:'arial', monospace; text-align: center; padding: 10px 0; border-radius:10px; width:70%; margin:auto \" >\n", "  <span style=\"font-size:20px;position:relative;color:white; \">  Note </span> <br>\n", "  <div style=\"background:#9de3a6;font-size:12px\"> \n", "    The confusion matrix can be an highly informative visualization tool to observe which classes are getting confused, and which classes show high discrimination.\n", "</div>\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": ["r = tree.export_text(clf, feature_names=best_features)\n", "print(r)\n", "print(tree.plot_tree(clf, feature_names = best_features, class_names=np.unique(Y), filled=True))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div style=\"background:#946db2;font-family:'arial', monospace; text-align: center; padding: 10px 0; border-radius:10px; width:70%; margin:auto \" >\n", "  <span style=\"font-size:20px;position:relative;color:white; \"> Explore </span> <br>\n", "  <div style=\"background:#d0b3e6;font-size:12px\"> \n", "    Explore defining some of its hyperparameters and the impact on the classification results. For more information go to: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier</div>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["###\n", "The <b>Naive Bayes</b> methods are a set of supervised learning algorithms based on applying Bayes\u2019 theorem with the \u201cnaive\u201d assumption of conditional independence between every pair of features given the value of the class variable. \n", "\n", "#### <div style=\"color:#fbb144\">   5.2.1.  Classification using Naive Bayes </div>\n", "\n", "We will implement the Gaussian Naive Bayes, which implements the Gaussian Naive Bayes algorithm for classification. The likelihood of the features is assumed to be Gaussian.\n", "\n", "naive_bayes.BernoulliNB(*[, alpha, \u2026]) -- Naive Bayes classifier for multivariate Bernoulli models.\n", "\n", "naive_bayes.CategoricalNB(*[, alpha, \u2026]) -- Naive Bayes classifier for categorical features\n", "\n", "naive_bayes.ComplementNB(*[, alpha, \u2026]) -- The Complement Naive Bayes classifier described in Rennie et al.\n", "\n", "naive_bayes.GaussianNB(*[, priors, \u2026]) -- Gaussian Naive Bayes (GaussianNB)\n", "\n", "naive_bayes.MultinomialNB(*[, alpha, \u2026]) -- Naive Bayes classifier for multinomial models"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# import the Gaussian naive bayes classifier\n", "from sklearn import naive_bayes  \n", "\n", "# call the classifier\n", "clf =  \n", "# Fit Gaussian Naive Bayes according to X_train, y_train\n", "clf =  \n", "# Perform classification on an array of test vectors X.\n", "y_predicted = \n", "\n", "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_predicted).sum()))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Print the classification report\n", "# print(classification_report(y_test, y_predicted))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Plot the Confusion Matrix\n", "target_names = np.unique(y_test)\n", "plot_confusion_matrix(y_test, y_predicted, true_labels=np.unique(y_test), normalize=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div style=\"background:#946db2;font-family:'arial', monospace; text-align: center; padding: 10px 0; border-radius:10px; width:70%; margin:auto \" >\n", "  <span style=\"font-size:20px;position:relative;color:white; \"> Explore </span> <br>\n", "  <div style=\"background:#d0b3e6;font-size:12px\"> \n", "    Explore diferent forms for the likelihood of the features and change some of its hyperparameters to observe the impact on the classification results. For more information go to: https://scikit-learn.org/stable/modules/naive_bayes.html\n", "        </div>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Compare the results against the previous classifiers:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# III. Explore\n", "<br>\n", "<div style=\"width:100%; background:linear-gradient(to right,#FDC86E,#fbb144);font-family:'arial black',monospace; text-align: center; padding: 7px 0; border-radius: 5px 50px;margin-top:-15px\" >  </div>\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## <div style=\"color:#fbb144;\">  1. Final Notes </div>\n", "\n", "In this notebook, we performed activity recognition on the data acquired through Google Science Journal through a comprehensive study of different supervised/unsupervised learning classifiers. The experimental results led to high accuracy results, thus paving the way to the development of systems capable of automatically identifying the activity solely based on accelerometer data."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## <div style=\"color:#fbb144;\">  2. Further Reading  </div>\n", "1. Explore how to acquire your own data [OpenSignals](../A.Signal_Acquisition/A001 Open Signals.ipynb)\n", "2. Explore signal-processing techniques to remove data noise [Noise](../C.Signal_Processing) \n", "3. Explore autoencoders to extract meaninful information from your data [AutoEncoders](../E.Classification/E001 Autoencoders Respiration.ipynb)\n", "4. Explore other notebooks: <br>[ScienceJournal](../A.Signal_Acquisition/ScienceJournal.ipynb) - Acquire signals through the mobile phone <br>\n", "[SignalClassification_using_SL](../E.Classification/E002 Signal Classification Using SL.ipynb)  - Machine Learning in Signal Classification \n", "[Hierarchical_Clustering] - Hierarchical Clustering of Activities"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div style=\"height:100px; background:white;border-radius:10px;text-align:center\"> \n", "\n", "<a> <img src=\"https://github.com/PIA-Group/ScientIST-notebooks/blob/master/_Resources/Images/IT.png?raw=true\" alt=\"it\" style=\" bottom: 0; width:250px;\n", "    display: inline;\n", "    left: 250px;\n", "    position: absolute;\"/> </a>\n", "<img src=\"https://github.com/PIA-Group/ScientIST-notebooks/blob/master/_Resources/Images/IST.png?raw=true\"\n", "         alt=\"alternate text\" \n", "         style=\"position: relative;   width:250px; float: left;\n", "    position: absolute;\n", "    display: inline;\n", "    bottom: 0;\n", "    right: 100;\"/>\n", "</div> "]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div style=\"width: 100%; \">\n", "<div style=\"background:linear-gradient(to right,#FDC86E,#fbb144);color:white;font-family:'arial', monospace; text-align: center; padding: 50px 0; border-radius:10px; height:10px; width:100%; float:left \" >\n", "<span style=\"font-size:12px;position:relative; top:-25px\">  Please provide us your feedback <span style=\"font-size:14px;position:relative;COLOR:WHITE\"> <a href=\"https://forms.gle/C8TdLQUAS9r8BNJM8\">here</a>.</span></span> \n", "<br>\n", "<span style=\"font-size:17px;position:relative; top:-20px\">  Suggestions are welcome! </span> \n", "</div>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["```Contributors: Mariana Abreu, Patr\u00edcia Bota```"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.6"}}, "nbformat": 4, "nbformat_minor": 4}